import os
from .utils import  remove_dir, load_task
import json
from openai import OpenAI
# from .utils import send_prompt, DyGraphPrompt, DyGraphGenERCon, load_task # ç¡®ä¿ load_task ç­‰å¯¼å…¥
from .utils import DyGraphGenControlMotif,DyGraphGenEdge, DyGraphPrompt, DyGraphGenERCon, DyGraphGenMotifCon,load_task, DyGraphGenMorMotifCon # ç¡®ä¿ load_task ç­‰å¯¼å…¥
from tqdm import tqdm
import numpy as np
import pandas as pd
import time
import openai # å¯¼å…¥ openai ä»¥ä¾¿æ•è·å…¶ç‰¹å®šé”™è¯¯
import torch # å¯¼å…¥ torch
from transformers import AutoModelForCausalLM, AutoTokenizer # å¯¼å…¥ transformers ç±»
from collections import defaultdict, Counter # å¯¼å…¥ Counter ç”¨äºé¢‘æ•°ç»Ÿè®¡
import langchain
from langchain.agents import initialize_agent, AgentType   
# --- æ·»åŠ å¯è§†åŒ–å¯¼å…¥ ---
import networkx as nx
from scipy.stats import entropy
from collections import Counter, defaultdict
try:
    from .utils import visualization
except ImportError:
    print("è­¦å‘Š: LLMDyG_Motif.utils.visualization æœªæ‰¾åˆ°æˆ–å¯¼å…¥å¤±è´¥ã€‚å¯è§†åŒ–åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")
    visualization = None
# --- æ·»åŠ ç»˜å›¾å¯¼å…¥ ---
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    plotting_available = True
except ImportError:
    print("è­¦å‘Š: matplotlib æˆ– seaborn æœªå®‰è£…ã€‚æ•°æ®åˆ†æç»˜å›¾åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")
    plotting_available = False
os.environ["NO_PROXY"] = "localhost,127.0.0.1"
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4,5,6,7"

# --- ä¿®å¤OpenAIå¼‚å¸¸å¯¼å…¥å…¼å®¹æ€§ ---
try:
    # æ–°ç‰ˆæœ¬OpenAIåº“ (v1.0+)
    from openai import RateLimitError, AuthenticationError, APIConnectionError
    OPENAI_RATE_LIMIT_ERROR = RateLimitError
    OPENAI_AUTH_ERROR = AuthenticationError
    OPENAI_CONNECTION_ERROR = APIConnectionError
except ImportError:
    try:
        # æ—§ç‰ˆæœ¬OpenAIåº“ (v0.x)
        from openai.error import RateLimitError, AuthenticationError, APIConnectionError
        OPENAI_RATE_LIMIT_ERROR = RateLimitError
        OPENAI_AUTH_ERROR = AuthenticationError
        OPENAI_CONNECTION_ERROR = APIConnectionError
    except ImportError:
        # å¦‚æœéƒ½å¯¼å…¥å¤±è´¥ï¼Œå®šä¹‰å…¼å®¹çš„å¼‚å¸¸ç±»
        class FallbackOpenAIError(Exception):
            pass
        OPENAI_RATE_LIMIT_ERROR = FallbackOpenAIError
        OPENAI_AUTH_ERROR = FallbackOpenAIError
        OPENAI_CONNECTION_ERROR = FallbackOpenAIError
        print("è­¦å‘Š: æ— æ³•å¯¼å…¥OpenAIå¼‚å¸¸ç±»ï¼Œä½¿ç”¨å…¼å®¹æ¨¡å¼")

import sys
import torch
import transformers
print("Python:", sys.version)
print("Torch:", torch.__version__)
print("Transformers:", transformers.__version__)
print("CUDA Available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("CUDA Version:", torch.version.cuda)
    # print("cuDNN Version:", torch.backends.cudnn.version()) # å¯èƒ½éœ€è¦ cudnn

PREDEFINED_MOTIFS = {
    "2-star":       [('u0', 'u1', 't0', 'a'), ('u0', 'u2', 't1', 'a')],                             # k=3, l=2
    "triangle":     [('u0', 'u1', 't0', 'a'), ('u1', 'u2', 't1', 'a'), ('u2', 'u0', 't2', 'a')],             # k=3, l=3
    "3-star":       [('u0', 'u1', 't0', 'a'), ('u0', 'u2', 't1', 'a'), ('u0', 'u3', 't2', 'a')],             # k=4, l=3 (ä¸­å¿ƒèŠ‚ç‚¹ä¸º 0)
    "4-path":       [('u0', 'u1', 't0', 'a'), ('u1', 'u2', 't1', 'a'), ('u2', 'u3', 't2', 'a')],             # k=4, l=3
    "4-cycle":      [('u0', 'u1', 't0', 'a'), ('u1', 'u2', 't1', 'a'), ('u2', 'u3', 't2', 'a'), ('u3', 'u0', 't3', 'a')], # k=4, l=4
    "4-tailedtriangle": [('u0', 'u1', 't0', 'a'), ('u1', 'u2', 't1', 'a'), ('u2', 'u3', 't2', 'a'), ('u3', 'u1', 't3', 'a')], # k=4, l=4
    "butterfly":    [('u0', 'u1', 't0', 'a'), ('u1', 'u3', 't1', 'a'), ('u3', 'u2', 't2', 'a'), ('u2', 'u0', 't3', 'a')], # k=4, l=4 (ä¸ 4-cycle æ‹“æ‰‘ç›¸åŒï¼Œæ—¶åºä¸åŒï¼Ÿæ ¹æ®å›¾åƒè§£ææœ‰æ­§ä¹‰ï¼Œæš‚ä¸åŒ…å«)
    "4-chordalcycle": [('u0', 'u1', 't0', 'a'), ('u1', 'u2', 't1', 'a'), ('u2', 'u3', 't2', 'a'), ('u3', 'u1', 't3', 'a'), ('u3', 'u0', 't4', 'a')], # k=4, l=5
    "4-clique":     [('u0', 'u1', 't0', 'a'), ('u1', 'u2', 't1', 'a'), ('u1', 'u3', 't2', 'a'), ('u2', 'u3', 't3', 'a'), ('u3', 'u0', 't4', 'a'), ('u0', 'u2', 't5', 'a')], # k=4, l=6
    "bitriangle":   [('u0', 'u1', 't0', 'a'), ('u1', 'u3', 't1', 'a'), ('u3', 'u5', 't2', 'a'), ('u5', 'u4', 't3', 'a'), ('u4', 'u2', 't4', 'a'), ('u2', 'u0', 't5', 'a')], # k=6, l=6
}


MODEL_PATHS = {
    "Qwen_14B": "LLMDyG_Motif/DeepSeek-R1-Distill-Qwen-14B",
    "Llama_8B": "LLMDyG_Motif/Llama-3.1-Nemotron-Nano-8B-v1",
    "Deepseek_7B": "LLMDyG_Motif/DeepSeek-R1-Distill-Qwen-7B",
    "Deepseek_7B_chat": "/LLMDyG_Motif/llama_2_7B_chat",
    "Llama_7B_chat": "/LLMDyG_Motif/llama_2_7B_chat",
    "Llama_13B_chat": "/LLMDyG_Motif/llama_2_13B_chat",
    "Qwen-32B-Chat": "/LLMDyG_Motif/Qwen1.5-32B-Chat-AWQ",
    "gpt-4o-mini": "",
    "deepseek-r1-250528": "",
    "QwQ": "/LLMDyG_Motif/QwQ-32B",
    "Qwen2.5_32B": "/LLMDyG_Motif/Qwen2.5-32B-Instruct",
    "Qwen_32B": "/LLMDyG_Motif/DeepSeek-R1-Distill-Qwen-32B",
    "DeepSeek-R1-Distill-Qwen-32B":"",
    "DeepSeek-R1-Distill-Qwen-14B":"",
    "o3-2025-04-16": "",
    "pangu_auto": "/home/ma-user/work/LLMDyG_Motif/Pangu",
}

def initialize_model(model):
    """åœ¨ä¸»è¿›ç¨‹ä¸­åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨"""
    model_path = MODEL_PATHS[model]
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model_obj = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto", # Changed to auto for simpler multi-GPU handling
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    model_obj.eval()
    return model_obj, tokenizer

def chat_with_model(model, prompt, temperature, max_tokens):
    """
    ä¸åŠ è½½çš„æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚
    """
    # å°† prompt åŒ…è£…æˆ messages æ ¼å¼ (å¦‚æœæ¨¡å‹éœ€è¦)
    # æ³¨æ„ï¼šQwen å¯èƒ½ä¸éœ€è¦è¿™ç§åŒ…è£…ï¼Œå¯ä»¥ç›´æ¥ç”¨ prompt å­—ç¬¦ä¸²ã€‚
    # ä½†ä¸ºäº†ä¸ send_prompt ä¿æŒæŸç§ç¨‹åº¦çš„ä¸€è‡´æ€§ï¼Œè¿™é‡Œä¿ç•™ messages ç»“æ„ï¼Œ
    # å¹¶åœ¨ tokenizer è°ƒç”¨æ—¶ç›´æ¥ä½¿ç”¨ promptã€‚
    # messages = [{"role": "user", "content": prompt}]

    torch.cuda.empty_cache() # æ¸…ç†ç¼“å­˜

    # inputs = tokenizer(
    #     prompt,
    #     return_tensors="pt",
    # )
    # prompt_tokens = inputs.input_ids.shape[1]


    try:
        debug_prompt_file = "prompt_debug_runner.txt"
        with open(debug_prompt_file, "w", encoding="utf-8") as f_debug:
            f_debug.write(prompt)
        print(f"[è°ƒè¯•ä¿¡æ¯] Prompt å·²ä¿å­˜åˆ°: {debug_prompt_file}")
    except Exception as e_debug:
        print(f"[è°ƒè¯•ä¿¡æ¯] ä¿å­˜ prompt åˆ°æ–‡ä»¶æ—¶å‡ºé”™: {e_debug}")

    # with torch.no_grad():
    #     # ç¡®ä¿ inputs åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    #     device_inputs = inputs.to(model.device)
    #     outputs = model.generate(
    #         **device_inputs,
    #         max_new_tokens=max_tokens,
    #         temperature=temperature,
    #         do_sample=False, # å¦‚æœ temperature > 0 æ‰è¿›è¡Œé‡‡æ ·
    #         pad_token_id=tokenizer.eos_token_id, # <--- æ·»åŠ æ˜¾å¼è®¾ç½®
    #         repetition_penalty=1.0           # <--- æ·»åŠ æ˜¾å¼è®¾ç½®
    #     )
    # completion_token_ids = outputs[0][prompt_tokens:]
    # response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # print(response_text)
    # completion_tokens = len(completion_token_ids)
    # total_tokens = prompt_tokens + completion_tokens

    # # æ¸…ç† GPU å†…å­˜
    # del inputs, outputs, completion_token_ids, device_inputs # æ¸…ç† device_inputs
    # torch.cuda.empty_cache()
    if model == "pangu_auto":
        client = OpenAI(
            api_key="sk-xxx",  # ä»»æ„å­—ç¬¦ä¸²å³å¯
            base_url="http://127.0.0.1:8888/v1", 
            default_headers={
                "Connection": "close",  # é¿å…ä¿æŒè¿æ¥
                "Keep-Alive": "timeout=0"  # ç¦ç”¨keep-alive
            }
        )
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=max_tokens,
        temperature=temperature
        # do_sample, pad_token_id, repetition_penalty éƒ½æ˜¯æœ¬åœ°æ¨¡å‹çš„å‚æ•°ï¼Œåœ¨APIä¸­ä¸æ”¯æŒ
    )
    response_text = response.choices[0].message.content
    prompt_tokens = response.usage.prompt_tokens
    completion_tokens = response.usage.completion_tokens
    total_tokens = response.usage.total_tokens

    result = {
        "content": response_text.strip(),
        "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens,
        "total_tokens": total_tokens
    }
    return result


# è¾…åŠ©å‡½æ•°ï¼šæ ¹æ® args ç”Ÿæˆå”¯ä¸€çš„ prompt é…ç½® ID
def _generate_prompt_suffix(args):
    """Generates a unique suffix based on prompt configuration arguments."""
    # ä½¿ç”¨ args.__dict__.get æä¾›é»˜è®¤å€¼ï¼Œé¿å… args ä¸­ç¼ºå°‘æŸäº›å±æ€§æ—¶å‡ºé”™
    k = args.__dict__.get('k', args.__dict__.get('num_examplars', 0)) # å…¼å®¹ k å’Œ num_examplars
    return (f"_cot{args.__dict__.get('add_cot', 0)}"
            f"_role{args.__dict__.get('add_role', 0)}"
            f"_k{k}"
            f"_dyg{args.__dict__.get('dyg_type', 0)}"
            f"_edge{args.__dict__.get('edge_type', 0)}"
            f"_imp{args.__dict__.get('imp', 0)}"
            f"_short{args.__dict__.get('short', 0)}"
            f"_temperature{args.__dict__.get('temperature', 0)}"
            f"_maxtokens{args.__dict__.get('max_tokens', 0)}"
            f"_motifname{args.__dict__.get('motif_name', 0)}"
            f"_motif{args.__dict__.get('motif', 0)}"
            f"_change{args.__dict__.get('change', 0)}"
            f"_use_agent{args.__dict__.get('use_agent', 0)}"
            f"_api{args.__dict__.get('api', 0)}"
            f"_balance{args.__dict__.get('balance', 0)}")
            

# å®šä¹‰ Runner ç±»ï¼Œè´Ÿè´£ç®¡ç†å’Œæ‰§è¡Œæ•´ä¸ªå®éªŒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
# 1. ç”Ÿæˆæ•°æ®å’Œé—®é¢˜ (gen) - åªç”Ÿæˆ graph.json å’Œ qa.json
# 2. è°ƒç”¨ LLM è·å–ç­”æ¡ˆ (run) - åŠ¨æ€ç”Ÿæˆ promptï¼Œä¿å­˜å¸¦åç¼€çš„ answer
# 3. è¯„ä¼°æ¨¡å‹æ€§èƒ½ (evaluate) - åŠ è½½å¸¦åç¼€çš„ answerï¼Œä¿å­˜å¸¦åç¼€çš„ results
# 4. æ£€æŸ¥è¿è¡ŒçŠ¶æ€ (check) - æ£€æŸ¥å¸¦åç¼€çš„ answer æ–‡ä»¶
# 5. æ±‡æ€»å’Œå±•ç¤ºç»“æœ (show)
class Runner:
    """
    ç®¡ç† LLM åœ¨åŠ¨æ€å›¾ä»»åŠ¡ä¸Šè¯„ä¼°çš„æ•´ä¸ªæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    1. ç”Ÿæˆæ•°æ®å’Œé—®é¢˜ (gen) - åªç”Ÿæˆ graph.json å’Œ qa.json
    2. è°ƒç”¨ LLM è·å–ç­”æ¡ˆ (run) - åŠ¨æ€ç”Ÿæˆ promptï¼Œä¿å­˜å¸¦åç¼€çš„ answer
    3. è¯„ä¼°æ¨¡å‹æ€§èƒ½ (evaluate) - åŠ è½½å¸¦åç¼€çš„ answerï¼Œä¿å­˜å¸¦åç¼€çš„ results
    4. æ£€æŸ¥è¿è¡ŒçŠ¶æ€ (check) - æ£€æŸ¥å¸¦åç¼€çš„ answer æ–‡ä»¶
    5. æ±‡æ€»å’Œå±•ç¤ºç»“æœ (show)
    """
    def __init__(self, args, try_all = False) -> None:
        """
        åˆå§‹åŒ– Runnerã€‚

        Args:
            args (object): åŒ…å«æ‰€æœ‰é…ç½®å‚æ•°çš„å¯¹è±¡ (é€šå¸¸æ¥è‡ª argparse)ã€‚
            try_all (bool, optional): æ˜¯å¦åœ¨ run æ¨¡å¼ä¸‹æŒç»­å°è¯•è¿è¡Œï¼Œç›´åˆ°æ‰€æœ‰ä»»åŠ¡å®Œæˆã€‚
                                      é»˜è®¤ä¸º Falseã€‚
        """
        self.args = args
        self.try_all = try_all
        
    def check(self, task_folder):
        """
        æ£€æŸ¥æŒ‡å®šä»»åŠ¡æ–‡ä»¶å¤¹ä¸­ç‰¹å®šé…ç½®çš„è¿è¡ŒçŠ¶æ€ã€‚

        Args:
            task_folder (str): ä»»åŠ¡æ•°æ®å’Œç»“æœå­˜å‚¨çš„æ ¹ç›®å½•ã€‚

        Returns:
            int: å¾…è¿è¡Œ (torun) çš„å®ä¾‹æ•°é‡ã€‚
        """
        args = self.args
        model = args.model
        prompt_suffix = _generate_prompt_suffix(args)

        # --- ä¿®æ”¹: åŠ è½½ instance_list.json ---
        instance_list_path = os.path.join(task_folder, "instance_list.json")
        try:
            with open(instance_list_path, "r") as f:
                 instance_info = json.load(f)
            instance_folders = instance_info["instance_list"] # ä½¿ç”¨æ–° key
            print(instance_folders)
        except FileNotFoundError:
             print(f"Error: instance_list.json not found in {task_folder}. Please run '-t gen' first.")
             return -1
        except json.JSONDecodeError:
             print(f"Error: instance_list.json in {task_folder} is corrupted.")
             return -1
        # --- ä¿®æ”¹ç»“æŸ ---

        finish = []
        torun = []
        sdict = {"num_edges": [], "num_nodes": [], "num_time": []}

        # --- ä¿®æ”¹: ä½¿ç”¨ instance_folders ---
        for i, folder_name in enumerate(instance_folders):
            folder_path = os.path.join(task_folder, folder_name)
            # åŠ è½½è¯¥å®ä¾‹çš„å›¾ä¿¡æ¯
            try:
                graph = json.load(open(os.path.join(folder_path, "graph.json")))
                # æ”¶é›†ç»Ÿè®¡æ•°æ®
                for k, v in sdict.items():
                    v.append(graph.get(k, 0)) # ä½¿ç”¨ get ä»¥é˜²æŸäº› key ä¸å­˜åœ¨
            except FileNotFoundError:
                print(f"Warning: graph.json not found in {folder_path}")
                torun.append(i) # å¦‚æœå›¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¹Ÿè§†ä¸ºå¾…è¿è¡Œ
                continue
            except json.JSONDecodeError:
                 print(f"Warning: graph.json in {folder_path} is corrupted.")
                 torun.append(i) # å¦‚æœå›¾æ–‡ä»¶æŸåï¼Œä¹Ÿè§†ä¸ºå¾…è¿è¡Œ
                 continue

            # --- ä¿®æ”¹: answer æ–‡ä»¶ç°åœ¨åœ¨æ¨¡å‹å­ç›®å½•ä¸­ ---
            model_subfolder_path = os.path.join(folder_path, model)
            answer_path = os.path.join(model_subfolder_path, f"answer_{model}{prompt_suffix}.json")
            # --- ä¿®æ”¹ç»“æŸ ---

            if os.path.exists(answer_path):
                try:
                    # å¯é€‰ï¼šæ£€æŸ¥ç­”æ¡ˆæ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
                    json.load(open(answer_path, "r"))
                    finish.append(i)
                except json.JSONDecodeError:
                    print(f"Warning: Answer file {answer_path} is corrupted.")
                    torun.append(i) # ç­”æ¡ˆæ–‡ä»¶æŸåï¼Œè§†ä¸ºå¾…è¿è¡Œ
            else:
                torun.append(i)
                
        print(f"--- Checking Status ---")
        print(f"Task Folder: {task_folder}")
        print(f"Model: {model}")
        print(f"Prompt Suffix: {prompt_suffix}")
        print(f"Finish {len(finish)}, ToRun {len(torun)} (Total {len(instance_folders)})")
        if sdict["num_edges"]: # ä»…åœ¨æ”¶é›†åˆ°æ•°æ®æ—¶æ‰“å°
             print("Graph Stats (AvgÂ±Std): " + "".join(f"{k}:{np.mean(v):.2f}Â±{np.std(v):.2f} " for k,v in sdict.items() if v))
        else:
             print("Graph Stats: No valid graph data found to compute statistics.")
        return len(torun)
        
    
    def generate_random(self, dir, T, N, p, seed, *targs ,**kwargs):
        """
        ä¸ºå•ä¸ªå®ä¾‹ç”ŸæˆåŠ¨æ€å›¾æ•°æ® (`graph.json`)ã€é—®ç­”å¯¹ (`qa.json`)ï¼Œ
        å¹¶å°†å®ƒä»¬ä¿å­˜åˆ°æŒ‡å®šçš„å­ç›®å½•ä¸­ã€‚**ä¸å†ä¿å­˜ prompt_qa.json**ã€‚

        Args:
            dir (str): ä¿å­˜è¯¥å®ä¾‹æ•°æ®çš„çˆ¶ç›®å½•ã€‚
            T (int): åŠ¨æ€å›¾çš„æ—¶é—´æ­¥æ•°ã€‚
            N (int): åŠ¨æ€å›¾çš„èŠ‚ç‚¹æ•°ã€‚
            p (float): åŠ¨æ€å›¾ç”Ÿæˆæ¨¡å‹çš„ç›¸å…³æ¦‚ç‡å‚æ•°ã€‚
            seed (int): ç”¨äºç”Ÿæˆè¯¥å®ä¾‹çš„éšæœºç§å­ã€‚
            *targs: ä¼ é€’ç»™ obj_task.generate_qa çš„é¢å¤–ä½ç½®å‚æ•°ã€‚
            **kwargs: ä¼ é€’ç»™ obj_task.generate_qa çš„é¢å¤–å…³é”®å­—å‚æ•° (ä¾‹å¦‚ label)ã€‚

        Returns:
            str: ç”Ÿæˆçš„å®ä¾‹çš„æ–‡ä»¶å¤¹åç§° (æ ¼å¼å¦‚ "T_N_p_seed")ã€‚
                 å¦‚æœç”Ÿæˆå¤±è´¥åˆ™è¿”å› Noneã€‚
        """
        args = self.args
        task = args.task

        if args.dataset != "random": # real-world dataset
            folder_setting = f"dyg{seed}"
        else:
            folder_setting = f"T{T}_N{N}_p{p}_seed{seed}"
        
        
        info = None
        if args.dataset != "random": # real-world dataset
            infos = json.load(open(f"/home/hb/LLMDyG_Motif/LLMDyG_Motif/dataset/{args.dataset}/{args.dataset}_subgraphs.json"))
            info = infos[args.motif_name].get(f"subgraphs", {}).get(f"{seed}", None)

        elif args.task == "judge_contain_motif":
            p = int(p)
            folder_setting = f"T{T}_N{N}_M{p}_seed{seed}"
            dygen = DyGraphGenControlMotif()
            info = dygen.generate_graph_with_motif_control(M = p, seed = seed, motif_name = args.motif_name, T = T)
        elif args.motif == 1 and task == "judge_motif":
            folder_setting = f"T{T}_N{N}_p{p}_seed{seed}"
            p = int(p)
            M = PREDEFINED_MOTIFS[args.motif_name]
            dygen = DyGraphGenMotifCon()
            info = dygen.sample_dynamic_graph(T + T, predefined_motif = M, motif_time_window = T, seed=seed)
        elif args.motif == 1 and task == "modify_dyg":
            folder_setting = f"T{T}_N{N}_p{p}_seed{seed}"
            target_motif = PREDEFINED_MOTIFS[args.motif_name]
            p = int(p)
            dygen = DyGraphGenMorMotifCon()
            info = dygen.sample_dynamic_graph(T_total_time = T, N_total_nodes = N , M_target_edges = p, W_motif_window = args.w[0], target_motif_definition = target_motif, seed = seed)
            if info is None:
                print(f"è­¦å‘Š (generate_random): æ•°æ®ç”Ÿæˆå¤±è´¥ (DyGraphGenMorMotifCon) for task '{task}', T={T}, N={N}, M={p}, W={args.w[0] if isinstance(args.w, list) else args.w}, seed={seed}. è·³è¿‡æ­¤å®ä¾‹.")
                return None # Return None to indicate failure for this seed
        else:
            dygen = DyGraphGenERCon()
            info = dygen.sample_dynamic_graph(T = T, N = N , p = p, seed = seed)
        obj_task = load_task(task, args)
        dygprompt = DyGraphPrompt(obj_task, args = args)
        
  
        # 2. æ ¹æ®å›¾ä¿¡æ¯å’Œä»»åŠ¡ç”Ÿæˆé—®ç­”å¯¹
        if info is None: # General check, covers cases if info is not set in other branches or if the specific check was missed
            print(f"è­¦å‘Š (generate_random): 'info' is None before QA generation for task '{task}', T={T}, N={N}, p/M={p}, seed={seed}. è·³è¿‡æ­¤å®ä¾‹.")
            return None
            
        qa = obj_task.generate_qa(info, *targs, **kwargs)

        # å¦‚æœ generate_qa è¿”å› None (ä¾‹å¦‚æ²¡æœ‰æœ‰æ•ˆæ•°æ®ç”Ÿæˆ QA)ï¼Œåˆ™ç›´æ¥è¿”å›
        if qa is None:
             print(f"è­¦å‘Š (generate_random): ä»»åŠ¡ {task} æœªèƒ½ä¸º T={T}, N={N}, p={p}, seed={seed} ç”Ÿæˆæœ‰æ•ˆçš„ QA æ•°æ®ã€‚è·³è¿‡æ­¤å®ä¾‹ã€‚")
             return None

        # --- ä¿å­˜æ–‡ä»¶ (æ¢å¤ try-except) ---
        instance_folder = os.path.join(dir, folder_setting) # å®ä¾‹æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        os.makedirs(instance_folder, exist_ok=True) # åˆ›å»ºå®ä¾‹æ–‡ä»¶å¤¹
        info_file = os.path.join(instance_folder, f"graph.json") # å›¾ä¿¡æ¯æ–‡ä»¶è·¯å¾„
        qa_file = os.path.join(instance_folder, f"qa.json") # é—®ç­”å¯¹æ–‡ä»¶è·¯å¾„
        
        # write files
        json.dump(info, open(info_file, "w"))

  
        if isinstance(qa.get("_original_context_set"), set):
            # å‡è®¾é›†åˆå…ƒç´ æ˜¯å…ƒç»„ï¼Œå¯ä»¥ç›´æ¥è½¬ä¸ºåˆ—è¡¨
            qa["_original_context_set"] = list(qa["_original_context_set"])
        if isinstance(qa.get("_final_edges_set"), set):
             # å‡è®¾é›†åˆå…ƒç´ æ˜¯å…ƒç»„ï¼Œå¯ä»¥ç›´æ¥è½¬ä¸ºåˆ—è¡¨
             qa["_final_edges_set"] = list(qa["_final_edges_set"])


        json.dump(qa, open(qa_file, "w"))

        # --- è°ƒç”¨å¯è§†åŒ– (ä¿æŒä¸å˜ï¼Œä½†åŒ…å«åœ¨ generate_random å†…) ---
        if visualization:
            try:
                vis_root_dir = os.path.join(dir, "visualizations")
                snapshot_dir = os.path.join(vis_root_dir, "snapshots")
                gif_dir = os.path.join(vis_root_dir, "gifs")
                os.makedirs(snapshot_dir, exist_ok=True)
                os.makedirs(gif_dir, exist_ok=True)

                snapshot_filename = f"snapshots_{folder_setting}.png"
                gif_filename = f"animation_{folder_setting}.gif"
                snapshot_path = os.path.join(snapshot_dir, snapshot_filename)
                gif_path = os.path.join(gif_dir, gif_filename)

                temporal_edges = info.get('edge_index')
                num_nodes = info.get('N')

                if temporal_edges and num_nodes is not None:
                    # print(f"  æ­£åœ¨ç”Ÿæˆå¯è§†åŒ– for {folder_setting}...") # å¯ä»¥å–æ¶ˆæ³¨é‡Šä»¥è·å–è¯¦ç»†æ—¥å¿—
                    visualization.visualize_graph(num_nodes, temporal_edges, snapshot_path)
                    visualization.create_colored_animation(num_nodes, temporal_edges, gif_path)
                else:
                    print(f"è­¦å‘Š (generate_random): ç¼ºå°‘å¯è§†åŒ–æ‰€éœ€æ•°æ® (edge_index or N) for {folder_setting}")

            except Exception as e: # ä¿æŒå¯¹å¯è§†åŒ–é”™è¯¯çš„æ•è·
                print(f"é”™è¯¯ (generate_random): è°ƒç”¨å¯è§†åŒ–å‡½æ•°æ—¶å‡ºé”™ for {folder_setting}: {e}")
                import traceback
                traceback.print_exc()

        return folder_setting

    

    # run
    def gen(self, dir):
        """
        ç”ŸæˆæŒ‡å®šä»»åŠ¡çš„æ‰€æœ‰é—®é¢˜å®ä¾‹æ•°æ® (`graph.json`, `qa.json`) åŠå¯è§†åŒ–ã€‚
        ç”Ÿæˆå®Œæˆåï¼Œè‡ªåŠ¨è¿›è¡Œæ—¶é—´æˆ³æ•°æ®åˆ†æã€‚
        """
        print(f'--- Generating Base Data Files & Visualizations ---')
        args = self.args
        os.makedirs(dir, exist_ok=True)
        # ä¿å­˜ gen args
        try:
            with open(os.path.join(dir, 'gen_args.json'), "w") as f:
                 json.dump(args.__dict__, f, indent=4)
        except Exception as e:
             print(f"Warning: Could not save gen_args.json: {e}")
        
        if args.dataset != "random": # real-world dataset
            instance_list = []
            task = args.task
            for i in range(20):
                folder_setting = self.generate_random(dir, args.T, args.N, args.p, seed=i)
                instance_list.append(folder_setting) # ä½¿ç”¨æ–°åˆ—è¡¨å
        else:

            instance_list = []
            label = 0
            task = args.task
            total_combinations = len(args.T) * len(args.N) * len(args.p)

            with tqdm(total=total_combinations * args.num_seed, desc="Generating Instances") as pbar:
                for T_val in args.T:
                    for N_val in args.N:
                        for p_val in args.p:
                            seed = 0
                            successful_seeds_for_combo = 0
                            attempts = 0
                            max_attempts = args.num_seed * 5 + 10
                            while successful_seeds_for_combo < args.num_seed and attempts < max_attempts:
                                current_seed = seed
                                if args.task == "judge_multi_motif":
                                    current_seed += 1
                                folder_setting = self.generate_random(dir, T_val, N_val, p_val, current_seed, label=label)
                                if folder_setting:
                                    instance_list.append(folder_setting) 
                                    successful_seeds_for_combo += 1
                                    pbar.update(1)
                                    label = 1 - label
                                else:
                                    print("folder_settingä¸ºç©º")
                                seed += 1
                                attempts += 1

                            if successful_seeds_for_combo < args.num_seed:
                                print(f"\nWarning: Generated {successful_seeds_for_combo}/{args.num_seed} instances for T={T_val}, N={N_val}, p={p_val} after {attempts} attempts.")

        instance_list_path = os.path.join(dir, f"instance_list.json")
        try:
            # ä½¿ç”¨ key "instance_list"
            with open(instance_list_path, "w") as f:
                json.dump({"instance_list": instance_list}, f)
            print(f"\nSuccessfully generated data for {len(instance_list)} instances.")
            print(f"instance_list.json saved in {dir}")
            if visualization:
                 print(f"Visualizations saved in {os.path.join(dir, 'visualizations')}")

            # --- æ·»åŠ åˆ†ææ­¥éª¤ (ä¾èµ– instance_list) ---
            if instance_list:
                 print("\n--- Starting Post-Generation Data Analysis ---")
                 self.analyze_data(dir)
            else:
                 print("\nSkipping data analysis as no instances were successfully generated.")
            # --- åˆ†ææ­¥éª¤ç»“æŸ ---

        except Exception as e:
             print(f"Error saving instance_list.json: {e}")
             print("\nSkipping data analysis due to error saving instance_list.json.")
        # --- ä¿®æ”¹ç»“æŸ ---
    
    def run_one(self, task_folder):
        """
        è¿è¡Œå•ä¸ªæ‰¹æ¬¡çš„ LLM è°ƒç”¨ï¼Œå¤„ç†æ‰€æœ‰å®ä¾‹é—®é¢˜ã€‚
        
        æ‰¹é‡å¤„ç†ç‹¬ç«‹æ€§ä¿è¯ï¼š
        1. Agentåˆå§‹åŒ–æ—¶è®¾ç½® disable_memory=True ç¦ç”¨å†…å­˜åŠŸèƒ½
        2. æ¯æ¬¡è°ƒç”¨ get_response æ—¶è®¾ç½® clear_history=True æ¸…ç©ºå†å²
        3. ç¡®ä¿100ä¸ªé—®é¢˜ä¹‹é—´å®Œå…¨æ²¡æœ‰è®°å¿†å…³è”
        
        å°† prompt_qa å’Œ answer æ–‡ä»¶éƒ½ä¿å­˜åˆ°æ¨¡å‹å­ç›®å½•ã€‚
        """
        args = self.args
        model = args.model
        use_agent = args.use_agent
        use_api = args.api
        
        if args.balance == 1:
            from .Balance.agent_manager import AgentManager
            agent_manager = AgentManager(
            model_name=model,
            temperature=0.1,
            max_new_tokens=10240,
            memory_k=5,
            verbose=True,
            max_iterations=5,  # Reduce maximum iterations to force Agent to finish quickly
            handle_parsing_errors=True,
            disable_memory=False
           )
        elif use_api == 1:
            from .api.api import OpenAIAPI
            api = OpenAIAPI(key=args.key)
        elif use_agent == 1:
            if args.model == "pangu_auto":
                from .agent_pangu.agent_manager import AgentManager
            else:
                from .agent_4o.agent_manager import AgentManager

            agent_manager = AgentManager(
                key=args.key,
                model_name=model,
                temperature=0.1,
                max_new_tokens=10240,
                memory_k=5,
                verbose=True,
                max_iterations=5,  # Reduce maximum iterations to force Agent to finish quickly
                handle_parsing_errors=True,
                disable_memory=False
            )

            print("ğŸ¤– åˆå§‹åŒ–Agent Managerç”¨äºæ‰¹é‡å¤„ç†...")
            
        # else:
            # model_obj, tokenizer = initialize_model(model)
        try:
            obj_task = load_task(args.task, args)
            dygprompt = DyGraphPrompt(obj_task, args=args)
        except Exception as e:
            print(f"Error initializing task or prompt generator: {e}")
            return

        prompt_suffix = _generate_prompt_suffix(args)

        # --- åŠ è½½ instance_list.json ---
        instance_list_path = os.path.join(task_folder, "instance_list.json")
        try:
            with open(instance_list_path, "r") as f:
                 instance_info = json.load(f)
            instance_folders = instance_info["instance_list"] # ä½¿ç”¨æ–° key
        except FileNotFoundError:
             print(f"Error: instance_list.json not found in {task_folder}. Cannot run.")
             return
        except json.JSONDecodeError:
             print(f"Error: instance_list.json in {task_folder} is corrupted. Cannot run.")
             return

        print(f"--- Running LLM Inference ---")
        print(f"Model: {model}, Prompt Suffix: {prompt_suffix}")

        average_times = 0
        average_tokens = 0
        with tqdm(instance_folders, desc="Processing Instances") as bar:
            for folder_name in bar:
                try:
                    folder_path = os.path.join(task_folder, folder_name)
                    qa_file_path = os.path.join(folder_path, "qa.json")

                    # --- ä¿®æ”¹: å®šä¹‰ prompt_qa å’Œ answer çš„è·¯å¾„ (éƒ½åœ¨æ¨¡å‹å­ç›®å½•) ---
                    model_subfolder_path = os.path.join(folder_path, model)
                    prompt_qa_filename = f"prompt_qa{prompt_suffix}.json"
                    prompt_qa_path = os.path.join(model_subfolder_path, prompt_qa_filename)

                    answer_filename = f"answer_{model}{prompt_suffix}.json"
                    answer_path = os.path.join(model_subfolder_path, answer_filename) # <--- ä¿®æ”¹ answer è·¯å¾„
                    
                    agent_filename = f"agent_{prompt_suffix}.log"
                    agent_log_path = os.path.join(model_subfolder_path, agent_filename)

                    balance_agent_filename = f"balance_agent_{prompt_suffix}.log"
                    balance_agent_log_path = os.path.join(model_subfolder_path, balance_agent_filename)
                    
                    api_filename = f"api_{prompt_suffix}.log"
                    api_log_path = os.path.join(model_subfolder_path, api_filename)


                    # æ£€æŸ¥ answer æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if os.path.exists(answer_path):
                        bar.set_postfix_str("Skipped (Answer Exists)")
                        continue

                    # åŠ è½½ qa æ•°æ®
                    try:
                        with open(qa_file_path, "r") as f:
                            qa = json.load(f)
                    except FileNotFoundError:
                         print(f"\nSkipping {folder_name}: qa.json not found.")
                         continue
                    except json.JSONDecodeError:
                         print(f"\nSkipping {folder_name}: qa.json is corrupted.")
                         continue

                    # åŠ¨æ€ç”Ÿæˆ prompt_qa å­—å…¸
                    prompt_qa = dygprompt.generate_prompt_qa(**qa)
                    prompt = prompt_qa['prompt']
                    # prompt = prompt.encode('utf-8').decode('unicode_escape')
                    # --- ä¿®æ”¹: ä¿å­˜ prompt_qa å’Œ answer å‰ç¡®ä¿æ¨¡å‹å­ç›®å½•å­˜åœ¨ ---
                    try:
                        os.makedirs(model_subfolder_path, exist_ok=True) # åˆ›å»ºæ¨¡å‹å­ç›®å½•
                    except OSError as e:
                         print(f"\nError creating model subfolder {model_subfolder_path}: {e}")
                         continue # æ— æ³•ä¿å­˜ï¼Œè·³è¿‡æ­¤å®ä¾‹

                    # ä¿å­˜ prompt_qa
                    try:
                        with open(prompt_qa_path, "w") as f:
                             json.dump(prompt_qa, f, indent=4)
                    except Exception as e:
                         print(f"\nWarning: Could not save prompt_qa file to {prompt_qa_path}: {e}")
                    
                    if args.balance == 1:
                        answer_content = agent_manager.get_response(
                        task="predict_llm_agent",
                        prompt=prompt, 
                        agent_log_path=balance_agent_log_path,
                        clear_history=True 
                      )
                        response_text = answer_content["content"].strip().lower()
                        if "llm" in response_text:
                            use_api = 1
                            use_agent = 0
                            from .api.api import OpenAIAPI
                            api = OpenAIAPI(key=args.key)
                        else:
                            use_api = 0
                            use_agent = 1
                            from .agent_pangu.agent_manager import AgentManager
                            agent_manager = AgentManager(
                                key=args.key,
                                model_name=model,
                                temperature=0.1,
                                max_new_tokens=10240,
                                memory_k=5,
                                verbose=True,
                                max_iterations=5,  # Reduce maximum iterations to force Agent to finish quickly
                                handle_parsing_errors=True,
                                disable_memory=False
                            )
                    if use_api == 1:
                        answer_content = api.get_response(model=model, prompt=prompt, api_log_path=api_log_path, max_tokens=args.max_tokens)
                        average_times += answer_content.get("duration", 0)
                        average_tokens += answer_content.get("usage", {}).get("total_tokens", 0)
                    elif use_agent == 1:
                        # ğŸ”‘ æ‰¹é‡å¤„ç†ï¼šæ¯ä¸ªé—®é¢˜éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œæ— è®°å¿†å…³è”
                        # get_responseæ–¹æ³•ä¼šè‡ªåŠ¨æ¸…ç©ºä»»ä½•æ®‹ç•™çš„è®°å¿†çŠ¶æ€
                        print(f"ğŸ”„ å¤„ç†ç‹¬ç«‹é—®é¢˜: {folder_name}")
                        answer_content = agent_manager.get_response(
                            task=args.task,
                            prompt=prompt, 
                            agent_log_path=agent_log_path,
                            clear_history=True  # æ˜ç¡®æŒ‡å®šæ¸…ç©ºå†å²ï¼Œç¡®ä¿é—®é¢˜ç‹¬ç«‹
                        )
                        average_times += answer_content.get("time", 0)
                        average_tokens += float(answer_content.get("tokens", 0).get("total_tokens", 0))
                        print("ğŸ“‹ Agentå“åº”å†…å®¹:")
                        print(answer_content["content"])
                        if args.verbose and answer_content.get("intermediate_steps"):
                            print("ğŸ”§ æ‰§è¡Œæ­¥éª¤:")
                            print(answer_content["intermediate_steps"]) 
                    # else:
                    #     # è°ƒç”¨ LLM
                    #     token_budget = con.get_token()
                    #     bar.set_postfix_str(f"Tokens Left: {int(token_budget)}")
                    #     # answer_content = chat_with_model(model_obj, tokenizer, prompt, temperature=args.temperature, max_tokens=args.max_tokens)
                    #     answer_content = chat_with_model(model, prompt, temperature=args.temperature, max_tokens=args.max_tokens)
                    #     con.time_token()
                    #     tokens_used = answer_content.get('total_tokens', 0)
                    #     con.use_token(tokens_used)
                    #     bar.set_postfix_str(f"Tokens Used: {tokens_used}")

                    # ä¿å­˜ç­”æ¡ˆ (åˆ°æ¨¡å‹å­ç›®å½•)
                    try:
                        with open(answer_path, "w") as f:
                             json.dump(answer_content, f) # ä¿å­˜åŒ…å« content å’Œ token çš„å®Œæ•´å­—å…¸
                    except Exception as e:
                         print(f"\nError saving answer file {answer_path}: {e}")

                except FileNotFoundError as e:
                    bar.set_postfix_str(f"Error: File not found - {e}")
                    print(f"\nSkipping {folder_name}: Required file not found - {e}")
                except json.JSONDecodeError as e:
                    # å®šä½å…·ä½“å“ªä¸ªæ–‡ä»¶è§£æå¤±è´¥
                    failed_file = "unknown json"
                    if 'qa_file_path' in locals() and qa_file_path in str(e): failed_file = qa_file_path
                    bar.set_postfix_str(f"Error: JSON decode - {failed_file}")
                    print(f"\nSkipping {folder_name}: JSON decode error in {failed_file} - {e}")
                except OPENAI_RATE_LIMIT_ERROR as e:
                    bar.set_postfix_str("Error: Rate Limit")
                    print(f"\nOpenAI Rate Limit Error encountered: {e}. Waiting and will retry (if using try_all)...")
                    time.sleep(60) # ç­‰å¾… 60 ç§’
                    # ä¾èµ–å¤–å±‚ try_all å¾ªç¯æ¥é‡è¯•å½“å‰å®ä¾‹
                    # æ³¨æ„ï¼šå¦‚æœä¸ç”¨ try_allï¼Œè¿™é‡Œä¼šå¯¼è‡´å®ä¾‹è¢«è·³è¿‡
                except OPENAI_AUTH_ERROR as e:
                     bar.set_postfix_str("Error: Auth Failed")
                     print(f"\nOpenAI Authentication Error: {e}. Check API key.")
                     # è¿™ç§é”™è¯¯é€šå¸¸æ— æ³•é€šè¿‡é‡è¯•è§£å†³ï¼Œå¯èƒ½éœ€è¦åœæ­¢
                     # break # å¯ä»¥é€‰æ‹©è·³å‡ºå¾ªç¯
                except OPENAI_CONNECTION_ERROR as e:
                     bar.set_postfix_str("Error: Connection")
                     print(f"\nOpenAI API Connection Error: {e}. Check network or server status. Waiting briefly...")
                     time.sleep(10) # çŸ­æš‚ç­‰å¾…åå¯èƒ½æ¢å¤
                     # ä¾èµ–å¤–å±‚ try_all é‡è¯•
                except Exception as e:
                    bar.set_postfix_str(f"Error: {type(e).__name__}")
                    print(f"\nError processing {folder_name}: {type(e).__name__} - {e}")
                    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†æˆ–æ—¥å¿—è®°å½•
                
        average_filename = f"average{prompt_suffix}.json"
        average_path = os.path.join(task_folder, average_filename)            
        with open(average_path, "w") as f:
            json.dump({"average_times": average_times/len(instance_folders), "average_tokens": average_tokens/len(instance_folders)}, f)
        print(f"Average times: {average_times/len(instance_folders)}, Average tokens: {average_tokens/len(instance_folders)}")

    def run(self, task_folder):
        """
        æ‰§è¡Œ LLM è°ƒç”¨ã€‚
        å¦‚æœ self.try_all ä¸º Trueï¼Œåˆ™ä¼šå¾ªç¯è°ƒç”¨ run_one ç›´åˆ°æ‰€æœ‰ä»»åŠ¡å®Œæˆã€‚
        å¦åˆ™åªè°ƒç”¨ä¸€æ¬¡ run_oneã€‚

        Args:
            task_folder (str): ä»»åŠ¡æ•°æ®å’Œç»“æœå­˜å‚¨çš„æ ¹ç›®å½•ã€‚
        """
        print(f"--- Starting LLM Run ---")
        print(f"Task: '{self.args.task}', Folder: {task_folder}")
        if self.try_all:
            while True:
                self.run_one(task_folder)
                print("\nChecking run status...")
                # æ£€æŸ¥ç‰¹å®šé…ç½®çš„å®Œæˆæƒ…å†µ
                torun = self.check(task_folder)
                if torun == 0:
                    print("All instances seem to be processed.")
                    break
                print("Continue Try to Run")
                time.sleep(5)
        else:
            self.run_one(task_folder)
            

    def evaluate(self, task_folder):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚
        """
        args = self.args
        model = args.model
        task = args.task
        prompt_suffix = _generate_prompt_suffix(args) # Suffix is needed to load correct answer file
        obj_task = load_task(task, args)


        instance_list_path = os.path.join(task_folder, "instance_list.json")
        try:
            with open(instance_list_path, "r") as f:
                 instance_info = json.load(f)
            instance_folders = instance_info["instance_list"]
        except FileNotFoundError:
             print(f"Error evaluating: instance_list.json not found in {task_folder}.")
             return
        except json.JSONDecodeError:
             print(f"Error evaluating: instance_list.json in {task_folder} is corrupted.")
             return


        metrics = []
        total_tokens = []
        prompt_tokens = []
        completion_tokens = []
        # Restore original error folder lists for simplicity as requested previously
        # temporal_wrong_folders = []
        # completion_wrong_folders = []
        wrong_folders = [] # æ·»åŠ ç”¨äºè®°å½• metric = 0 (é”™è¯¯) çš„åˆ—è¡¨
        fail_folders = [] # æ·»åŠ ç”¨äºè®°å½• metric = -1 (å¤±è´¥/è§£æé”™è¯¯) çš„åˆ—è¡¨
        num_times = []
        num_edges = []
        num_nodes = []

        print(f"--- Evaluating Results ---")
        # --- ä¿®æ”¹: ä½¿ç”¨åç¼€æ‰“å° ---
        print(f"Task: {task}, Model: {model}, Suffix: {prompt_suffix}")
        # instance_folders = instance_folders[:68]
        # --- ä¿®æ”¹: ä½¿ç”¨ instance_folders ---
        for folder_name in tqdm(instance_folders, desc="Evaluating Instances"):
            folder_path = os.path.join(task_folder, folder_name)
            qa_file_path = os.path.join(folder_path, "qa.json")
            graph_path = os.path.join(folder_path, f"graph.json")
            # --- ä¿®æ”¹: ä»æ¨¡å‹å­ç›®å½•åŠ è½½ answer æ–‡ä»¶ ---
            model_subfolder_path = os.path.join(folder_path, model)
            answer_path = os.path.join(model_subfolder_path, f"answer_{model}{prompt_suffix}.json")
            with open(qa_file_path, "r") as f:
                qa = json.load(f)   
            with open(answer_path, "r") as f:
                answer = json.load(f) # answer is the dict with 'content', 'total_tokens' etc.
            with open(graph_path, "r") as f:
                graph = json.load(f)

            # ç°åœ¨ qa ä¸­çš„é›†åˆå·²ç»è¢«æ¢å¤ï¼Œå¯ä»¥å®‰å…¨ä¼ é€’ç»™ obj_task.evaluate

            if args.task == "modify_dyg":
                metric, extracted_answer = obj_task.evaluate(qa, answer["content"], args.w[0])
            else:
                # å®‰å…¨åœ°ä¼ é€’use_agentå‚æ•°ï¼Œé¿å…ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‡ºé”™
                try:
                    metric, extracted_answer = obj_task.evaluate(qa, answer["content"], use_agent=args.use_agent)
                except TypeError:
                    # å¦‚æœä»»åŠ¡ç±»ä¸æ”¯æŒuse_agentå‚æ•°ï¼Œåˆ™ä¸ä¼ é€’è¯¥å‚æ•°
                    metric, extracted_answer = obj_task.evaluate(qa, answer["content"])
            metrics.append(metric)
            
            # --- æ¢å¤: åŸå§‹é”™è¯¯åˆ†ç±» (åŒ…å« fail_folders) ---
            if metric < 0:
                fail_folders.append(folder_name) # è§£æ/æ ¼å¼é”™è¯¯
            elif metric == 0 or metric == 2:
               wrong_folders.append(folder_name) # å›å¤é”™è¯¯
            # --- æ¢å¤: è®°å½• token å’Œ stats ---
            total_tokens.append(answer.get('total_tokens', 0))
            prompt_tokens.append(answer.get('prompt_tokens', 0))
            completion_tokens.append(answer.get("completion_tokens", 0))
            num_times.append(graph.get('num_time', 0))
            num_edges.append(graph.get('num_edges', 0))
            num_nodes.append(graph.get('num_nodes', 0))
        # --- æ¢å¤: åŸå§‹ç»Ÿè®¡è®¡ç®— (ä¿®æ”¹ä¸ºåŒ…å« fail_rate) ---
        num_all = len(metrics)
        if num_all == 0:
            print("é”™è¯¯ï¼šæœªèƒ½æˆåŠŸè¯„ä¼°ä»»ä½•å®ä¾‹ã€‚")
            return
        print(num_all)
        print(metrics)
        right_rate = sum(metric for metric in metrics if metric > 0 and metric <= 1) / num_all if num_all > 0 else 0
        # é”™è¯¯ç‡ï¼šmetric = 0, -1, -2 (é€»è¾‘é”™è¯¯ã€æ’åºé”™è¯¯ã€å®Œæ•´æ€§é”™è¯¯)
        wrong_rate = sum(1 for m in metrics if m == 0 or m == 2) / num_all if num_all > 0 else 0
        # å¤±è´¥ç‡ï¼šmetric = -3 (è§£æé”™è¯¯ã€æ–‡ä»¶æœªæ‰¾åˆ°ã€JSONé”™è¯¯ã€è¯„ä¼°ä¸­æ„å¤–é”™è¯¯)
        fail_rate = sum(1 for m in metrics if m == -1) / num_all if num_all > 0 else 0

        total_tokens_sum = sum(t for t in total_tokens if t is not None) # ç¡®ä¿åŠ æ€»æ—¶å¿½ç•¥ None
        valid_token_counts = len([t for t in total_tokens if t is not None])
        average_tokens_calc = total_tokens_sum / valid_token_counts if valid_token_counts > 0 else 0

        # --- æ¢å¤: åŸå§‹ results ç»“æ„ (æ›´æ–°é”™è¯¯/å¤±è´¥ç‡) ---
        results = {
            "task": task,
            "model": model,
            "prompt_config_suffix": prompt_suffix,
            "correct_rate": right_rate, # æ­£ç¡®ç‡ (metric=1)
            "error_rate": wrong_rate,   # é”™è¯¯ç‡ (metric=0, -1, -2)
            "failure_rate": fail_rate,  # å¤±è´¥ç‡ (metric=-3)
            "average_tokens": average_tokens_calc,
            "total_tokens": total_tokens_sum,
            "average_num_times": np.mean(num_times) if num_times else 0,
            "average_num_edges": np.mean(num_edges) if num_edges else 0,
            "average_num_nodes": np.mean(num_nodes) if num_nodes else 0,
            "metrics": metrics, # åŸå§‹ metric åˆ—è¡¨
            "total_tokens_list": total_tokens,
            "prompt_tokens_list": prompt_tokens,
            "completion_tokens_list": completion_tokens,
            "error_folders": wrong_folders, # åˆå¹¶é€»è¾‘é”™è¯¯å’Œå®Œæ•´æ€§é”™è¯¯
            "failure_folders": fail_folders, # å•ç‹¬åˆ—å‡ºå¤±è´¥çš„æ–‡ä»¶å¤¹
            # ä¿ç•™æ—§çš„åˆ†ç±»ï¼Œå¦‚æœéœ€è¦å‘åå…¼å®¹
            # "temporal_wrong_folders": temporal_wrong_folders,
            # "completion_wrong_folders": completion_wrong_folders,
        }

        # --- ä¿®æ”¹: ä¿å­˜å¸¦åç¼€çš„ results æ–‡ä»¶ ---
        results_filename = f"results_{args.motif_name}_{model}{prompt_suffix}.json"
        results_save_path = os.path.join(task_folder, results_filename)
        try:
             with open(results_save_path, "w") as f:
                 json.dump(results, f, indent=4)
             print(f"\nEvaluation results saved to: {results_save_path}")
        except Exception as e:
            print(f"\nError saving evaluation results to {results_save_path}: {e}")

        # --- æ¢å¤: åŸå§‹æ‰“å°æ ¼å¼ (æ›´æ–°é”™è¯¯/å¤±è´¥ç‡) ---
        print(f"\n--- Evaluation Summary ---")
        print(f"Task: {task}, Model: {model}, Suffix: {prompt_suffix}") # æ·»åŠ  Suffix
        # æ›´æ–°æ‰“å°è¾“å‡ºä»¥åæ˜ æ–°çš„ç‡
        print(f"Correct Rate: {right_rate:.4f}, Error Rate: {wrong_rate:.4f}, Failure Rate: {fail_rate:.4f}")
        print(f"Average Tokens: {average_tokens_calc:.2f}, Total Tokens: {total_tokens_sum}")
        # æ¢å¤ np.std è®¡ç®—
        time_std = np.std(num_times) if num_times else 0
        edge_std = np.std(num_edges) if num_edges else 0
        node_std = np.std(num_nodes) if num_nodes else 0
        print(f"Num_time : {results['average_num_times']:.2f}+-{time_std:.2f} Num_edges : {results['average_num_edges']:.2f}+-{edge_std:.2f} Num Nodes : {results['average_num_nodes']:.2f}+-{node_std:.2f}")
        print(f"Total Instances Evaluated: {num_all}")
        if args.task == "sort_edge":
            print("--------------------------------")
            print("is_complete and not is_sorted: ", sum(1 for m in metrics if m == 2) / num_all if num_all > 0 else 0)
            print("not_complete and is_sorted: ", sum(1 for m in metrics if m == 3) / num_all if num_all > 0 else 0)
            print("not_complete and not is_sorted: ", sum(1 for m in metrics if m == 4) / num_all if num_all > 0 else 0)
            print("--------------------------------")

    def show(self, dir):
        """
        å±•ç¤ºå•ä¸ªä»»åŠ¡çš„ç»“æœã€‚(åŠ è½½å¸¦åç¼€çš„ç»“æœæ–‡ä»¶)
        """
        args = self.args
        task = args.task
        task_folder = args.task_folder
        model = args.model
        prompt_suffix = _generate_prompt_suffix(args)
        # --- ä¿®æ”¹: åŠ è½½å¸¦åç¼€çš„ç»“æœæ–‡ä»¶ ---
        results_file = os.path.join(task_folder, f"results_{model}{prompt_suffix}.json")

        print(f"--- Showing Results ---") 
        print(f"Attempting to load: {results_file}")

        try:
             with open(results_file, "r") as f:
                  results_data = json.load(f)
        except FileNotFoundError:
             print(f"Error: Results file not found. Please ensure evaluation was run with matching parameters.")
             return
        except json.JSONDecodeError:
             print(f"Error: Results file is corrupted.")
             return

        # --- æ¢å¤: æ‰“å°æ›´å¤šä¿¡æ¯ ---
        print(f"\n--- Results Summary (from {os.path.basename(results_file)}) ---")
        print(f"Task: {results_data.get('task', 'N/A')}")
        print(f"Model: {results_data.get('model', 'N/A')}")
        print(f"Suffix: {results_data.get('prompt_config_suffix', 'N/A')}") # æ˜¾ç¤ºåç¼€
        print(f"Average Accuracy (metric>0): {results_data.get('average_acc', 'N/A'):.4f}")
        print(f"Wrong Rate (metric<0): {results_data.get('wrong_rate', 'N/A'):.2f}")
        print(f"Average Tokens: {results_data.get('average_tokens', 'N/A'):.2f}")
        print(f"Total Tokens: {results_data.get('total_tokens', 'N/A')}")
        # print(f"Temporal Errors (metric=-1): {len(results_data.get('temporal_wrong_folders', []))}")
        # print(f"Completion Errors (metric=-2): {len(results_data.get('completion_wrong_folders', []))}")

        # --- æ¢å¤: ç§»é™¤åˆ†ç»„é€»è¾‘è¯´æ˜ ---
        # print("\nNote: Detailed breakdown by T, N, p requires adapting the 'show' method or storing more data in results.")


    def analyze_data(self, task_folder):
        """
        åˆ†æç”Ÿæˆæ•°æ®çš„æ—¶é—´æˆ³ã€èŠ‚ç‚¹æ•°ã€è¾¹æ•°åˆ†å¸ƒï¼Œå¹¶å°†ç»Ÿè®¡ç»“æœå’Œå¯è§†åŒ–å›¾ä¿å­˜ã€‚
        """
        if not plotting_available:
            print("é”™è¯¯: ç»˜å›¾åº“ (matplotlib/seaborn) ä¸å¯ç”¨ã€‚æ— æ³•æ‰§è¡Œåˆ†æã€‚")
            return

        args = self.args
        instance_list_path = os.path.join(task_folder, "instance_list.json")
        if not os.path.exists(instance_list_path):
             print(f"Error analyzing: instance_list.json not found in {task_folder}.")
             return
        try:
            with open(instance_list_path, "r") as f:
                 instance_info = json.load(f)
            instance_folders = instance_info["instance_list"]
        except json.JSONDecodeError:
             print(f"Error analyzing: instance_list.json in {task_folder} is corrupted.")
             return

        # --- ä¿®æ”¹: åˆå§‹åŒ–æ‰€æœ‰åˆ—è¡¨ ---
        all_timestamps = []
        all_node_counts = []
        all_edge_counts = []
        # --- ä¿®æ”¹ç»“æŸ ---
        valid_instances = 0
        folders_to_analyze = instance_folders

        print(f"Analyzing graph properties from {len(folders_to_analyze)} instances...")
        for folder_name in tqdm(folders_to_analyze, desc="Analyzing Graphs"): # æ›´æ–°æè¿°
            graph_path = os.path.join(task_folder, folder_name, "graph.json")
            try:
                with open(graph_path, "r") as f:
                    graph_data = json.load(f)

                # --- ä¿®æ”¹: æ”¶é›†èŠ‚ç‚¹æ•°å’Œè¾¹æ•° ---
                node_count = graph_data.get('N', None)
                edge_count = graph_data.get('num_edges', None) # num_edges é€šå¸¸ç­‰äº len(edge_index)
                timestamps_in_file = [int(edge[2]) for edge in graph_data.get('edge_index', []) if len(edge) > 2] # ç¡®ä¿è¾¹å…ƒç»„æœ‰æ•ˆ

                if node_count is not None and edge_count is not None:
                    all_node_counts.append(node_count)
                    all_edge_counts.append(edge_count)
                    all_timestamps.extend(timestamps_in_file) # åªæœ‰åœ¨èŠ‚ç‚¹å’Œè¾¹æœ‰æ•ˆæ—¶æ‰æ·»åŠ æ—¶é—´æˆ³ï¼Ÿæˆ–è€…åˆ†å¼€å¤„ç†ï¼Ÿåˆ†å¼€å¤„ç†æ›´å¥½
                    valid_instances += 1
                else:
                    print(f"\nWarning: Skipping instance {folder_name} due to missing 'N' or 'num_edges' in graph.json.")
                # --- ä¿®æ”¹ç»“æŸ ---

            except FileNotFoundError:
                 print(f"\nWarning: graph.json not found for instance {folder_name}. Skipping.")
            except json.JSONDecodeError:
                 print(f"\nWarning: graph.json for instance {folder_name} is corrupted. Skipping.")
            except Exception as e:
                 print(f"\nWarning: Error processing {folder_name} during analysis: {e}")

        # --- ä¿®æ”¹: è¿‡æ»¤ None å€¼ (å°½ç®¡ä¸Šé¢çš„é€»è¾‘å¯èƒ½å·²ç»é¿å…äº†ï¼Œä½†ä¸ºäº†å®‰å…¨) ---
        valid_nodes = [n for n in all_node_counts if n is not None]
        valid_edges = [e for e in all_edge_counts if e is not None]
        valid_timestamps = [t for t in all_timestamps if t is not None] # ç¡®ä¿æ—¶é—´æˆ³åˆ—è¡¨ä¹Ÿå¹²å‡€
        # --- ä¿®æ”¹ç»“æŸ ---

        if not valid_nodes or not valid_edges or not valid_timestamps:
            print(f"é”™è¯¯: æœªèƒ½ä»ä»»ä½•æœ‰æ•ˆå®ä¾‹ä¸­æ”¶é›†åˆ°è¶³å¤Ÿçš„å›¾å±æ€§æ•°æ® (éœ€è¦èŠ‚ç‚¹ã€è¾¹å’Œæ—¶é—´æˆ³)ã€‚æœ‰æ•ˆå®ä¾‹æ•°: {valid_instances}")
            return

        print(f"åˆ†æå®Œæˆã€‚ä» {valid_instances}/{len(instance_folders)} ä¸ªæœ‰æ•ˆå®ä¾‹ä¸­æ”¶é›†åˆ°æ•°æ®ã€‚")
        print(f"  èŠ‚ç‚¹æ•°: {len(valid_nodes)}, è¾¹æ•°: {len(valid_edges)}, æ—¶é—´æˆ³äº‹ä»¶: {len(valid_timestamps)}")

        # --- ä¿®æ”¹: è®¡ç®—æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯ ---
        stats_data = {
            "nodes": {
                "count": len(valid_nodes),
                "min": int(np.min(valid_nodes)) if valid_nodes else 0,
                "max": int(np.max(valid_nodes)) if valid_nodes else 0,
                "mean": float(np.mean(valid_nodes)) if valid_nodes else 0,
                "std": float(np.std(valid_nodes)) if valid_nodes else 0,
            },
            "edges": {
                "count": len(valid_edges),
                "min": int(np.min(valid_edges)) if valid_edges else 0,
                "max": int(np.max(valid_edges)) if valid_edges else 0,
                "mean": float(np.mean(valid_edges)) if valid_edges else 0,
                "std": float(np.std(valid_edges)) if valid_edges else 0,
            },
            "timestamps": {
                "count": len(valid_timestamps),
                "min": int(np.min(valid_timestamps)) if valid_timestamps else 0,
                "max": int(np.max(valid_timestamps)) if valid_timestamps else 0,
                "mean": float(np.mean(valid_timestamps)) if valid_timestamps else 0,
                "std": float(np.std(valid_timestamps)) if valid_timestamps else 0,
                "counts_per_step": dict(Counter(valid_timestamps)) if valid_timestamps else {}
            }
        }
        # --- ä¿®æ”¹ç»“æŸ ---

        try:
            vis_dir = os.path.join(task_folder, "visualizations")
            os.makedirs(vis_dir, exist_ok=True)

            # --- ä¿®æ”¹: ä¿å­˜ç»„åˆç»Ÿè®¡æ–‡ä»¶ ---
            stats_save_path = os.path.join(vis_dir, "graph_analysis_stats.json")
            with open(stats_save_path, "w") as f:
                json.dump(stats_data, f, indent=4)
            print(f"å›¾å±æ€§ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜è‡³: {stats_save_path}")
            # --- ä¿®æ”¹ç»“æŸ ---

            # --- ä¿®æ”¹: åˆ›å»ºç»„åˆç»˜å›¾ ---
            fig, axes = plt.subplots(1, 3, figsize=(18, 5)) # 1 è¡Œ 3 åˆ—

            # å­å›¾ 1: èŠ‚ç‚¹æ•°åˆ†å¸ƒ
            sns.histplot(valid_nodes, ax=axes[0], kde=False, discrete=True)
            axes[0].set_title('Node Count Distribution')
            axes[0].set_xlabel('Number of Nodes (N)')
            axes[0].set_ylabel('Frequency')
            max_node = stats_data["nodes"]["max"]
            if max_node <= 30:
                 axes[0].set_xticks(range(stats_data["nodes"]["min"], max_node + 1))
            elif max_node > 0:
                 tick_step = max(1, max_node // 10)
                 axes[0].set_xticks(range(stats_data["nodes"]["min"], max_node + tick_step, tick_step))


            # å­å›¾ 2: è¾¹æ•°åˆ†å¸ƒ
            sns.histplot(valid_edges, ax=axes[1], kde=True) # ä½¿ç”¨ KDE å¯ä»¥çœ‹åˆ°æ›´å¹³æ»‘çš„åˆ†å¸ƒ
            axes[1].set_title('Edge Count Distribution')
            axes[1].set_xlabel('Number of Edges')
            axes[1].set_ylabel('Frequency / Density')

            # å­å›¾ 3: æ—¶é—´æˆ³åˆ†å¸ƒ (ä¿ç•™åŸå§‹ç›´æ–¹å›¾é€»è¾‘)
            max_T = stats_data["timestamps"]["max"] + 1 if valid_timestamps else 1
            bins = range(max_T + 1) if max_T > 1 else [0, 1]
            sns.histplot(valid_timestamps, bins=bins, kde=False, stat="count", discrete=True, ax=axes[2])
            axes[2].set_title('Timestamp Distribution (Event Count)')
            axes[2].set_xlabel('Timestamp (t)')
            axes[2].set_ylabel('Number of Events')
            if max_T <= 20 and max_T > 1:
                 axes[2].set_xticks(bins[:-1])
            elif max_T > 1:
                 tick_step = max(1, (max_T - 1) // 10)
                 axes[2].set_xticks(range(0, max_T, tick_step))


            plt.tight_layout()
            plot_filename = os.path.join(vis_dir, "graph_analysis_distributions.png") # æ–°æ–‡ä»¶å
            plt.savefig(plot_filename, dpi=150)
            print(f"ç»„åˆå›¾å±æ€§åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {plot_filename}") # æ›´æ–°æ‰“å°ä¿¡æ¯
            plt.close(fig) # å…³é—­å›¾å½¢ï¼Œé‡Šæ”¾å†…å­˜
            # --- ä¿®æ”¹ç»“æŸ ---

        except Exception as e:
            print(f"é”™è¯¯: å¤„ç†ç»Ÿè®¡æˆ–ç»˜å›¾æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc() # æ‰“å°è¯¦ç»†é”™è¯¯è¿½è¸ª
        
    def execute(self, dir):
        """æ ¹æ® args.t æ‰§è¡Œç›¸åº”çš„æ“ä½œã€‚"""
        args = self.args
        task_folder = args.task_folder
        if args.dataset != "random":
            task_folder = os.path.join(task_folder, args.dataset)
        if args.task == "judge_motif" or args.task == "modify_dyg" or args.task == "judge_contain_motif":
            task_folder = os.path.join(task_folder, args.motif_name)
        # if args.task == "judge_contain_motif":    
        #     task_folder = os.path.join(task_folder, f"M{args.p}")
        if args.api == 1:
            task_folder = os.path.join(task_folder, "api")
        if args.use_agent == 1:
            task_folder = os.path.join(task_folder, "agent")
        if args.balance == 1:
            task_folder = os.path.join(task_folder, "balance")
        print(f"\nExecuting action '{args.t}' for task '{args.task}' in folder '{task_folder}'")
        if args.t == "clear":
            remove_dir(task_folder)
        elif args.t == "gen":
            self.gen(task_folder) # gen æ–¹æ³•ç°åœ¨ä¼šè‡ªåŠ¨è°ƒç”¨ analyze_data
        elif args.t == "run":
            self.run(task_folder)
        elif args.t == "eval":
            self.evaluate(task_folder)
        elif args.t == "check":
            self.check(task_folder)
        elif args.t == "show":
            self.show(task_folder)
        # --- æ·»åŠ : æ˜¾å¼è°ƒç”¨ analyze ---
        elif args.t == "analyze":
            print(f"\n--- Explicitly running Data Analysis ---")
            self.analyze_data(task_folder)
        # --- æ·»åŠ ç»“æŸ ---
        else:
            print(f"Error: Action '{args.t}' not implemented.")
            raise NotImplementedError

